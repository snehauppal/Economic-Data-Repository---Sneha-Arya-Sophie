# A: Architecture & Design Decisions 
# The main goal of this project was to create a domain-specific RAG chatbot capable of answering questions about economic datasets and trends. We chose to examine economic data because the economic indicators like unemployment rates and tourism expenditure, are precise numerical values that are easy to work with and have specific output values. This is well suited for the RAG pipeline because it relies on a direct answer rather than the model's general knowledge. We were able to incorporate our ETL work from our previous project and economic data is extremely relevant right now based on its use in policy. Given the structure of our dataset, we did not implement a specific chunking strategy since we used a structured CSV. Instead, we set our dataset up so that each query retrieves specific values from the cleaned CSV dataset. This ensures that the data that the model retrieves is accurate to the source. However, this may not translate to larger datasets and models. 

# B: Retrieval Quality & Failure Analysis
# For the first successful example, we asked the question, what was the unemployment rate for the US in 2019? The model then retrieved the unemployment rate of 3.7 from the cleaned CSV. It generated the response, “The unemployment rate in the United States in 2019 were 3.7.” This query demonstrates proper retrieval because it pulls the relevant country, year, and percentage from the dataset. 
# For the second successful example, we asked the question, what was the tourist expenditure (USD) for Australia in 2018? The model then retrieved the tourism expenditure rate 47,259.8 USD from the cleaned CSV. It generated the response, “The tourism expenditure (USD) in Australia in 2018 was 47,259.8.” This query demonstrates proper retrieval because it pulls the relevant country, year, and dollar amount from the dataset. 
# One example of a failure was asking a question about the GDP. We did not have a defined GDP column in our cleaned CSV dataset, so this did not have an output. The question we could have asked was what was the GDP growth rate for the United States in 2019? However, the system would not be able to answer this question because the data does not exist. This failure shows that there are limitations to what the system can answer based on the data it receives. 

# C: API & Engineering Challenges 
# An API is useful because it provides a way for people to interact with a specific pipeline without needing direct access to the complicated code and data. However, when working with APIs many challenges can arise. One challenge that we ran into was transferring the CSV ETL data to the virtual machine. Our Mac’s download folder was causing issues, so we switched to using the scp command to transfer the file. However, this had to be adapted to the private student key we used for the virtual machine earlier on in this class. Additionally, we had to adjust the keys and repository access. We had trouble cloning the GitHub repository because of the SSH key setup and permission issues. In order for the FastAPI server to work we had to activate the correct virtual environment which was easy to forget. Making sure the virtual environment was active before running uvicorn fixed this issue. While coding, we had to adjust app.py to make sure the API endpoint showed up the way it was supposed to. We ran into some case sensitivity errors when running the request model and request body. Making the capitalization consistent resolved this issue. We also struggled with permission errors with GitHub when trying to upload the API code. 
# We also ran into some engineering challenges when working on the RAG pipeline. The LLM model we used was too large for the computer, and it frequently led to crashing. We fixed this by modifying certain parameters, however, this slowed the coding down. We also struggled with the pipeline giving the wrong answers to questions because it was not querying documents correctly. In order to fix this issue, we had to make the rag_query function more specific to using pandas to try and get it to fetch the right data. While this was not the most successful RAG pipeline and LLM, we figured out how to get it to fetch what it needs for this project.

# D: Team Collaboration & Process 
# We started our project by meeting over Zoom to divide up the work. Sneha took parts 1 and 2, focusing on ETL integration and the RAG  pipeline. We were able to use our ETL CSV from our previous project, making it much easier and quicker to incorporate into our final project. Sneha worked through all the components of the RAG pipeline, preparing it to run the FastAPI. Next Arya worked on part 3, the FastAPI. She worked through this process and uploaded her work to the GitHub repository. Finally, Sophie worked on parts 4 and 5, focusing on organizational structure and the reflection. She took the work she was given and broke it up into sections to upload it neatly to the GitHub repository. She also used what the other team members told her about their parts of the project to write a concise reflection outlining the final project. We utilized iMessage to communicate, texting whenever we ran into an issue or needed to communicate a project update. This project taught us a lot about how difficult it is to get a RAG pipeline up and running. We cannot imagine how difficult it is to create an AI machine that can answer almost anything. Overall, this process was a great learning experience. 
